{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba12a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 457 verbs remain to be added to the stimuli lists. 250 adjectives remain to be added to the stimuli lists.\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2       able\n",
       "298    acrid\n",
       "22     acute\n",
       "159    adept\n",
       "300     agro\n",
       "       ...  \n",
       "231    windy\n",
       "229     wiry\n",
       "158    wiser\n",
       "182    witty\n",
       "21     worst\n",
       "Name: word, Length: 250, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "271    abide\n",
       "218    ached\n",
       "440    aches\n",
       "113    acted\n",
       "154    adapt\n",
       "       ...  \n",
       "87      wore\n",
       "387     wove\n",
       "220    woven\n",
       "41     wrote\n",
       "384    wrung\n",
       "Name: word, Length: 457, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "REMOVE = (\n",
    "    \"was\",\n",
    "    \"did\",\n",
    "    \"has\",\n",
    "    \"had\",\n",
    "    \"vary\",\n",
    "    \"that\",\n",
    "    \"these\",\n",
    "    \"those\",\n",
    "    \"with\",\n",
    "    \"him\",\n",
    "    \"her\",\n",
    "    \"them\",\n",
    "    \"us\",\n",
    "    \"me\",\n",
    "    \"you\",\n",
    "    \"he\",\n",
    "    \"she\",\n",
    "    \"it\",\n",
    "    \"they\",\n",
    "    \"we\",\n",
    "    \"my\",\n",
    "    \"your\",\n",
    "    \"his\",\n",
    "    \"her\",\n",
    "    \"its\",\n",
    "    \"our\",\n",
    "    \"their\",\n",
    "    \"out\",\n",
    "    \"for\",\n",
    "    \"they\",\n",
    "    \"then\",\n",
    "    \"yells\",\n",
    ")\n",
    "\n",
    "\n",
    "# Load in Nina's final stimuli lists and the consonants\n",
    "stimpath = Path(\"../../word_ngrams/\").resolve()\n",
    "pnp_stim_even = pd.read_csv(stimpath / \"final_P_NP_evensubs.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "pnp_stim_odd = pd.read_csv(stimpath / \"final_P_NP_oddsubs.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "pnp_stim = pd.concat([pnp_stim_even, pnp_stim_odd]).reset_index(drop=True)\n",
    "cons = pd.read_csv(stimpath / \"cons_clust_final_candidates.csv\").set_index([\"clus1\", \"clus2\"])\n",
    "\n",
    "# Load in the core list of all words, isolate by nouns\n",
    "wordcats = pd.read_csv(stimpath / \"1grams_english_1b_with_pos.csv\").convert_dtypes()\n",
    "\n",
    "# Isolate the words which are nouns in the top 45k\n",
    "wordcats[\"word\"] = wordcats[\"ngram\"].str.split(\"_\").str[0]\n",
    "wordcats = wordcats[~wordcats[\"word\"].str.isupper()].reset_index(drop=True)\n",
    "wordcats[\"POS\"] = wordcats[\"ngram\"].str.split(\"_\").str[1]\n",
    "wordcats.drop(columns=[\"ngram\"], inplace=True)\n",
    "wordcats = wordcats.reindex(columns=[\"word\", \"POS\", \"freq\"])\n",
    "verbs = wordcats[wordcats[\"POS\"] == \"VERB\"].reset_index(drop=True)\n",
    "verbs[\"word\"] = verbs[\"word\"].str.encode(\"ascii\", errors=\"ignore\").str.decode(\"ascii\")\n",
    "verbs[\"wordlen\"] = verbs[\"word\"].str.len()\n",
    "\n",
    "adjs = wordcats[wordcats[\"POS\"] == \"ADJ\"].reset_index(drop=True)\n",
    "adjs[\"word\"] = adjs[\"word\"].str.encode(\"ascii\", errors=\"ignore\").str.decode(\"ascii\")\n",
    "adjs[\"wordlen\"] = adjs[\"word\"].str.len()\n",
    "\n",
    "wccopy = wordcats.copy()\n",
    "wccopy[\"word\"] = wccopy[\"word\"].str.lower()\n",
    "wccopy[\"wordlen\"] = wccopy[\"word\"].str.len()\n",
    "wccopy = wccopy.loc[wccopy[\"wordlen\"].isin((3, 4, 5))]\n",
    "allverbs = wccopy.query(\"POS == 'VERB'\")[\"word\"].unique()\n",
    "alladjs = wccopy.query(\"POS == 'ADJ'\")[\"word\"].unique()\n",
    "ncats = wccopy.set_index(\"word\").sort_index().groupby(\"word\").nunique(\"POS\")\n",
    "goodverbs = ncats.loc[allverbs].query(\"POS == 1\").reset_index(drop=False)[\"word\"].unique()\n",
    "goodadjs = ncats.loc[alladjs].query(\"POS == 1\").reset_index(drop=False)[\"word\"].unique()\n",
    "verbs = verbs.loc[verbs[\"word\"].isin(goodverbs)]\n",
    "adjs = adjs.loc[adjs[\"word\"].isin(goodadjs)]\n",
    "\n",
    "# Isolate the nouns not yet in the stimuli lists\n",
    "pnp_stim_words = pd.melt(pnp_stim[[\"w1\", \"w2\"]], ignore_index=False).reset_index(drop=False)\n",
    "remidx = ~verbs[\"word\"].isin(pnp_stim_words[\"value\"])\n",
    "adj_remidx = ~adjs[\"word\"].isin(pnp_stim_words[\"value\"])\n",
    "candidates = verbs[remidx & verbs[\"wordlen\"].isin((3, 4, 5))].reset_index(drop=True).copy()\n",
    "adj_candidates = adjs[adj_remidx & adjs[\"wordlen\"].isin((3, 4, 5))].reset_index(drop=True).copy()\n",
    "\n",
    "adj_candidates = adjs.reset_index(drop=True)[\"word\"].sort_values().str.lower().copy()\n",
    "adj_candidates = adj_candidates[\n",
    "    ~adj_candidates.isin(REMOVE)\n",
    "    & ~adj_candidates.isin(candidates)\n",
    "    & adj_candidates.str.isalpha()\n",
    "    & ~adj_candidates.isin(pnp_stim_words[\"value\"])\n",
    "    & adj_candidates.str.len().isin((3, 4, 5))\n",
    "]\n",
    "\n",
    "candidates = candidates.reset_index(drop=True)[\"word\"].sort_values().str.lower().copy()\n",
    "candidates = candidates[\n",
    "    ~candidates.isin(REMOVE)\n",
    "    & ~candidates.isin(adj_candidates)\n",
    "    & candidates.str.isalpha()\n",
    "    & ~candidates.isin(pnp_stim_words[\"value\"])\n",
    "    & candidates.str.len().isin((3, 4, 5))\n",
    "]\n",
    "print(\n",
    "    f\"A total of {len(candidates)} verbs remain to be added to the stimuli lists.\"\n",
    "    f\" {len(adj_candidates)} adjectives remain to be added to the stimuli lists.\"\n",
    ")\n",
    "print(candidates.isin(pnp_stim_words[\"value\"]).sum())\n",
    "display(adj_candidates)\n",
    "display(candidates)\n",
    "print(candidates.isin(REMOVE).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2721fea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114250, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/cg6jvt750xl8q9tkh9t3fhfh0000gn/T/ipykernel_97118/1315638746.py:19: DeprecationWarning: `is_in` with a collection of the same datatype is ambiguous and deprecated.\n",
      "Please use `implode` to return to previous behavior.\n",
      "\n",
      "See https://github.com/pola-rs/polars/issues/22149 for more information.\n",
      "  goodcand = q.collect().to_pandas()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>able</td>\n",
       "      <td>abide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58944</th>\n",
       "      <td>lunar</td>\n",
       "      <td>abide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59401</th>\n",
       "      <td>lurid</td>\n",
       "      <td>abide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59858</th>\n",
       "      <td>manic</td>\n",
       "      <td>abide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60315</th>\n",
       "      <td>manly</td>\n",
       "      <td>abide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63056</th>\n",
       "      <td>muddy</td>\n",
       "      <td>wrung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26042</th>\n",
       "      <td>eerie</td>\n",
       "      <td>wrung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12333</th>\n",
       "      <td>brisk</td>\n",
       "      <td>wrung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62599</th>\n",
       "      <td>molar</td>\n",
       "      <td>wrung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114240</th>\n",
       "      <td>worst</td>\n",
       "      <td>wrung</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114241 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           w1     w2\n",
       "0        able  abide\n",
       "58944   lunar  abide\n",
       "59401   lurid  abide\n",
       "59858   manic  abide\n",
       "60315   manly  abide\n",
       "...       ...    ...\n",
       "63056   muddy  wrung\n",
       "26042   eerie  wrung\n",
       "12333   brisk  wrung\n",
       "62599   molar  wrung\n",
       "114240  worst  wrung\n",
       "\n",
       "[114241 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Convert the candidates to a polars DataFrame for faster processing\n",
    "\n",
    "verb_cand = pl.from_pandas(candidates.str.lower().copy())\n",
    "adj_cand = pl.from_pandas(adj_candidates.str.lower().copy())\n",
    "# Create a list of all possible combinations of nouns and adjectives\n",
    "pairs = adj_cand.to_frame().join(verb_cand.to_frame(), how=\"cross\")\n",
    "pairs = pairs.rename({\"word\": \"w1\", \"word_right\": \"w2\"})\n",
    "display(pairs.shape)\n",
    "\n",
    "\n",
    "# Load 2grams\n",
    "ngrams2 = pl.read_csv(stimpath / \"2grams_english_1a_no_pos.csv\")\n",
    "ngrams2.with_columns(ngram=ngrams2[\"ngram\"].str.to_lowercase())\n",
    "\n",
    "q = pairs.lazy().filter(\n",
    "    pl.concat_str(pl.col(\"w1\"), pl.col(\"w2\"), separator=\" \").is_in(ngrams2[\"ngram\"]).not_(),\n",
    "    pl.concat_str(pl.col(\"w2\"), pl.col(\"w1\"), separator=\" \").is_in(ngrams2[\"ngram\"]).not_(),\n",
    ")\n",
    "goodcand = q.collect().to_pandas()\n",
    "display(goodcand.sort_values(\"w2\"))\n",
    "\n",
    "goodcand.to_csv(stimpath / \"addtl_stimuli_phrase_candidates.csv\", index=False)\n",
    "print(goodcand[\"w1\"].isin(pnp_stim_words[\"value\"]).sum())\n",
    "print(goodcand[\"w2\"].isin(pnp_stim_words[\"value\"]).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8516942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/cg6jvt750xl8q9tkh9t3fhfh0000gn/T/ipykernel_97118/3480220159.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  freq_df = finalcand.replace(wordcounts)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>condition</th>\n",
       "      <th>subtype</th>\n",
       "      <th>w1_type</th>\n",
       "      <th>w2_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deep</td>\n",
       "      <td>blue</td>\n",
       "      <td>phrase</td>\n",
       "      <td>odd</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iced</td>\n",
       "      <td>tea</td>\n",
       "      <td>phrase</td>\n",
       "      <td>odd</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just</td>\n",
       "      <td>ahead</td>\n",
       "      <td>phrase</td>\n",
       "      <td>odd</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eight</td>\n",
       "      <td>weeks</td>\n",
       "      <td>phrase</td>\n",
       "      <td>odd</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst</td>\n",
       "      <td>thing</td>\n",
       "      <td>phrase</td>\n",
       "      <td>odd</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>newer</td>\n",
       "      <td>cater</td>\n",
       "      <td>non-phrase</td>\n",
       "      <td>odd</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>gated</td>\n",
       "      <td>infer</td>\n",
       "      <td>non-phrase</td>\n",
       "      <td>odd</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>limp</td>\n",
       "      <td>fared</td>\n",
       "      <td>non-phrase</td>\n",
       "      <td>odd</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>inert</td>\n",
       "      <td>shuts</td>\n",
       "      <td>non-phrase</td>\n",
       "      <td>odd</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>tidal</td>\n",
       "      <td>dares</td>\n",
       "      <td>non-phrase</td>\n",
       "      <td>odd</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        w1     w2   condition subtype w1_type w2_type\n",
       "0     deep   blue      phrase     odd    word    word\n",
       "1     iced    tea      phrase     odd    word    word\n",
       "2     just  ahead      phrase     odd    word    word\n",
       "3    eight  weeks      phrase     odd    word    word\n",
       "4    worst  thing      phrase     odd    word    word\n",
       "..     ...    ...         ...     ...     ...     ...\n",
       "115  newer  cater  non-phrase     odd    word    word\n",
       "116  gated  infer  non-phrase     odd    word    word\n",
       "117   limp  fared  non-phrase     odd    word    word\n",
       "118  inert  shuts  non-phrase     odd    word    word\n",
       "119  tidal  dares  non-phrase     odd    word    word\n",
       "\n",
       "[120 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/cg6jvt750xl8q9tkh9t3fhfh0000gn/T/ipykernel_97118/3480220159.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nw_cand[\"word\"] = nw_cand[\"word\"].str.lower()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>wordlen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1751648374</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>had</td>\n",
       "      <td>VERB</td>\n",
       "      <td>851806969</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>were</td>\n",
       "      <td>VERB</td>\n",
       "      <td>593516792</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>been</td>\n",
       "      <td>VERB</td>\n",
       "      <td>419558529</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>has</td>\n",
       "      <td>VERB</td>\n",
       "      <td>394883282</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8112</th>\n",
       "      <td>boxed</td>\n",
       "      <td>VERB</td>\n",
       "      <td>214275</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8197</th>\n",
       "      <td>savor</td>\n",
       "      <td>VERB</td>\n",
       "      <td>210186</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201</th>\n",
       "      <td>clap</td>\n",
       "      <td>VERB</td>\n",
       "      <td>210116</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8204</th>\n",
       "      <td>glean</td>\n",
       "      <td>VERB</td>\n",
       "      <td>209811</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8248</th>\n",
       "      <td>holed</td>\n",
       "      <td>VERB</td>\n",
       "      <td>208124</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word   POS        freq  wordlen\n",
       "1       was  VERB  1751648374        3\n",
       "4       had  VERB   851806969        3\n",
       "6      were  VERB   593516792        4\n",
       "10     been  VERB   419558529        4\n",
       "12      has  VERB   394883282        3\n",
       "...     ...   ...         ...      ...\n",
       "8112  boxed  VERB      214275        5\n",
       "8197  savor  VERB      210186        5\n",
       "8201   clap  VERB      210116        4\n",
       "8204  glean  VERB      209811        5\n",
       "8248  holed  VERB      208124        5\n",
       "\n",
       "[342 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take the hand-chosen final phrase candidates\n",
    "finalcand = pd.read_csv(stimpath / \"handpicked_phrase_new.csv\")\n",
    "npfinal = pd.read_csv(stimpath / \"handpicked_adjverb_phrase_new.csv\")\n",
    "wordcounts = finalcand.melt()[\"value\"].value_counts()\n",
    "freq_df = finalcand.replace(wordcounts)\n",
    "# display(finalcand)\n",
    "# display(finalcand.set_index(\"w2\").sort_index())\n",
    "\n",
    "# Randomly assign, except where the final word is duplicated: then split on the first word\n",
    "rng = np.random.default_rng(42)\n",
    "words_even = []\n",
    "words_odd = []\n",
    "pairs = finalcand.set_index(\"w2\", append=True).sort_index()\n",
    "ukcounts = pairs.index.get_level_values(\"w2\").value_counts().sort_values(ascending=False)\n",
    "ukeys = ukcounts.index.tolist()\n",
    "\n",
    "for ukey in ukeys:\n",
    "    keypairs = pairs.xs(ukey, level=\"w2\")\n",
    "    match (len(keypairs), len(words_even), len(words_odd)):\n",
    "        case 1, 60, _:\n",
    "            words_odd.append((keypairs.iloc[0][\"w1\"], ukey))\n",
    "        case 1, _, 60:\n",
    "            words_even.append((keypairs.iloc[0][\"w1\"], ukey))\n",
    "        case 1, _, _:\n",
    "            if rng.random() < 0.5:\n",
    "                words_odd.append((keypairs.iloc[0][\"w1\"], ukey))\n",
    "            else:\n",
    "                words_even.append((keypairs.iloc[0][\"w1\"], ukey))\n",
    "        case 2, _, _:\n",
    "            idx1 = 1 if rng.random() < 0.5 else 0\n",
    "            idx2 = 0 if idx1 == 1 else 1\n",
    "            words_even.append((keypairs.iloc[idx1][\"w1\"], ukey))\n",
    "            words_odd.append((keypairs.iloc[idx2][\"w1\"], ukey))\n",
    "        case _, _, _:\n",
    "            raise ValueError(f\"Unexpected number of words: {len(keypairs)} for {ukey}\")\n",
    "\n",
    "evenstim = pd.DataFrame(np.array(words_even), columns=[\"w1\", \"w2\"])\n",
    "evenstim[\"condition\"] = \"phrase\"\n",
    "evenstim[\"subtype\"] = \"even\"\n",
    "evenstim[\"w1_type\"] = \"word\"\n",
    "evenstim[\"w2_type\"] = \"word\"\n",
    "oddstim = pd.DataFrame(np.array(words_odd), columns=[\"w1\", \"w2\"])\n",
    "oddstim[\"condition\"] = \"phrase\"\n",
    "oddstim[\"subtype\"] = \"odd\"\n",
    "oddstim[\"w1_type\"] = \"word\"\n",
    "oddstim[\"w2_type\"] = \"word\"\n",
    "# display(evenstim)\n",
    "# display(oddstim)\n",
    "# display(evenstim.melt()[\"value\"].value_counts().head(4))\n",
    "# display(oddstim.melt()[\"value\"].value_counts().head(4))\n",
    "\n",
    "npfinal[\"condition\"] = \"non-phrase\"\n",
    "npfinal[\"subtype\"] = \"even\"\n",
    "npfinal[\"w1_type\"] = \"word\"\n",
    "npfinal[\"w2_type\"] = \"word\"\n",
    "\n",
    "npfinalsamp = npfinal.sample(120).reset_index(drop=True)\n",
    "npfinalsamp.loc[60:, \"subtype\"] = \"odd\"\n",
    "\n",
    "evenstim = pd.concat([\n",
    "    evenstim,\n",
    "    npfinalsamp.iloc[:60],\n",
    "]).reset_index(drop=True)\n",
    "oddstim = pd.concat([\n",
    "    oddstim,\n",
    "    npfinalsamp.iloc[60:],\n",
    "]).reset_index(drop=True)\n",
    "display(oddstim)\n",
    "\n",
    "allstim = pd.concat([\n",
    "    evenstim.melt(id_vars=[], value_vars=[\"w1\", \"w2\"]),\n",
    "    oddstim.melt(id_vars=[], value_vars=[\"w1\", \"w2\"]),\n",
    "    pnp_stim_words,\n",
    "])[\"value\"]\n",
    "nw_cand = verbs[~verbs[\"word\"].str.lower().isin(allstim) & verbs[\"wordlen\"].isin((3, 4, 5))]\n",
    "nw_cand[\"word\"] = nw_cand[\"word\"].str.lower()\n",
    "\n",
    "nwstim = pd.read_csv(stimpath / \"final_nw_stimuli.csv\", index_col=0).rename({\n",
    "    \"w1\": \"w2\",\n",
    "    \"w2\": \"w1\",\n",
    "    \"w1_len\": \"w2_len\",\n",
    "    \"w2_len\": \"w1_len\",\n",
    "    \"w1_type\": \"w2_type\",\n",
    "    \"w2_type\": \"w1_type\",\n",
    "})\n",
    "display(nw_cand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a251797",
   "metadata": {},
   "outputs": [],
   "source": [
    "even_df = pd.read_csv(stimpath.parent / \"stimuli\" / \"new_even_two_word_stimuli.csv\")\n",
    "odd_df = pd.read_csv(stimpath.parent / \"stimuli\" / \"new_odd_two_word_stimuli.csv\")\n",
    "even_df.loc[60:119] = evenstim.loc[60:119]\n",
    "odd_df.loc[60:119] = oddstim.loc[60:119]\n",
    "\n",
    "even_df.to_csv(stimpath.parent / \"stimuli\" / \"new_even_two_word_stimuli.csv\", index=False)\n",
    "odd_df.to_csv(stimpath.parent / \"stimuli\" / \"new_odd_two_word_stimuli.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264120e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
